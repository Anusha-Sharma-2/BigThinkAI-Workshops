# RAG + FastLanguageModel Workshop ðŸ’¡ðŸ“š

In this workshop, we build a powerful AI research assistant using Retrieval-Augmented Generation (RAG), FastLanguageModel, and LangChain. 

The project intends to teach RAG fundamentals in the context of conducting research, starting with one broad question and using recursive decomposition to generate further questions, which we answer using our upgraded LLM.

---

## What It Covers
- **Prompt Engineering for LLMs**  
  What is Prompt Engineering and tech side of how LLM architecture depends on prompts
- **RAG (Retrieval-Augmented Generation)**  
  Understand the RAG pipeline, its motives, what vector embeddings are and how they differ from keyword search, and vector databases such as Milvus
- **LangChain for LLM Workflows**  
  What are LangChain Chains, Using LangChain as a modular framework for building LLM apps
- **FastLanguageModel (Unsloth)**  
  What this is and why we use it
